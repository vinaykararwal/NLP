{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"ml-ymca.ipynb","version":"0.3.2","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"EHzIN9T_06XN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"aa87caa1-c86b-4c23-ff97-195a4d48da2a","executionInfo":{"status":"ok","timestamp":1567945773816,"user_tz":-330,"elapsed":2984,"user":{"displayName":"vinay kararwa l","photoUrl":"","userId":"11223157673134585915"}}},"source":["#command\n","\"\"\"\n","Shift + Enter to RUN the Cell\n","\"\"\""],"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nShift + Enter to RUN the Cell\\n'"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"code","metadata":{"id":"8xLIH2zL06XV","colab_type":"code","colab":{}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import h5py\n","import scipy\n","from PIL import Image\n","from scipy import ndimage\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lrd5OnY306XZ","colab_type":"code","colab":{}},"source":["def load_dataset():\n","    train_dataset = h5py.File('/content/gdrive/My Drive/ml-ymca/datasets/train_catvnoncat.h5', \"r\")\n","    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n","    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n","\n","    test_dataset = h5py.File('/content/gdrive/My Drive/ml-ymca/datasets/test_catvnoncat.h5', \"r\")\n","    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n","    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n","\n","    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n","    \n","    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n","    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n","    \n","    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IWaRptZ506Xe","colab_type":"code","colab":{}},"source":["train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eM7k00xk06X0","colab_type":"code","colab":{}},"source":["# Example of a picture\n","index = 25\n","plt.imshow(train_set_x_orig[index])\n","print (\"y = \" + str(train_set_y[:, index]) + \", it's a '\" + classes[np.squeeze(train_set_y[:, index])].decode(\"utf-8\") + \"picture.\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sfk2Riy706X_","colab_type":"code","colab":{}},"source":["### START CODE HERE ### (� 3 lines of code)\n","m_train = train_set_x_orig.shape[0]\n","m_test = test_set_x_orig.shape[0]\n","num_px = train_set_x_orig.shape[1]\n","### END CODE HERE ###\n","print (\"Number of training examples: m_train = \" + str(m_train))\n","print (\"Number of testing examples: m_test = \" + str(m_test))\n","print (\"Height/Width of each image: num_px = \" + str(num_px))\n","print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n","print (\"train_set_x shape: \" + str(train_set_x_orig.shape))\n","print (\"train_set_y shape: \" + str(train_set_y.shape))\n","print (\"test_set_x shape: \" + str(test_set_x_orig.shape))\n","print (\"test_set_y shape: \" + str(test_set_y.shape))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rIrP-U5S06Ya","colab_type":"code","colab":{}},"source":["# Reshape the training and test examples\n","### START CODE HERE ### (� 2 lines of code)\n","train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0],-1).T\n","test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0],-1).T\n","### END CODE HERE ###\n","print (\"train_set_x_flatten shape: \" + str(train_set_x_flatten.shape))\n","print (\"train_set_y shape: \" + str(train_set_y.shape))\n","print (\"test_set_x_flatten shape: \" + str(test_set_x_flatten.shape))\n","print (\"test_set_y shape: \" + str(test_set_y.shape))\n","print (\"sanity check after reshaping: \" + str(train_set_x_flatten[0:5,0]))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yuCPERw-06Ym","colab_type":"code","colab":{}},"source":["train_set_x = train_set_x_flatten/255.\n","test_set_x = test_set_x_flatten/255."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-_sFl9Co06Yq","colab_type":"code","colab":{}},"source":["# Write code for sigmoid function\n","def sigmoid(z):\n","    #Start writing your code\n","    s=1/(np.exp(-1*z)+1)\n","    #End writing your code\n","    return s"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3oAFd9Pc06Yv","colab_type":"code","colab":{}},"source":["# GRADED FUNCTION: initialize_with_zeros\n","def initialize_with_zeros(dim):\n","    w = np.zeros((dim,1))\n","    b = 0\n","    assert(w.shape == (dim, 1))\n","    assert(isinstance(b, float) or isinstance(b, int))\n","    return w, b"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jxeIWZt606Z7","colab_type":"code","colab":{}},"source":["# GRADED FUNCTION: propagate\n","def propagate(w, b, X, Y):\n","    m = X.shape[1]\n","    A = sigmoid(np.dot(w.T,X)+b)\n","    cost = -(np.dot(Y,np.log(A.T))+np.dot(np.log(1-A),(1-Y).T))/m\n","\n","    dw = np.dot(X,(A-Y).T)/m\n","    db = np.sum(A-Y)/m\n","\n","    assert(dw.shape == w.shape)\n","    assert(db.dtype == float)\n","    cost = np.squeeze(cost)\n","    assert(cost.shape == ())\n","    grads = {\"dw\": dw,\n","            \"db\": db}\n","    return grads, cost"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2nGRk79I06a1","colab_type":"code","colab":{}},"source":["# GRADED FUNCTION: optimize\n","def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n","    \"\"\"\n","    This function optimizes w and b by running a gradient descent\n","    , →\n","    algorithm\n","    , →\n","    , →\n","    , →\n","    , →\n","    , →\n","    , →\n","    Arguments:\n","    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n","    b -- bias, a scalar\n","    X -- data of shape (num_px * num_px * 3, number of examples)\n","    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of\n","    shape (1, number of examples)\n","    num_iterations -- number of iterations of the optimization loop\n","    learning_rate -- learning rate of the gradient descent update rule\n","    print_cost -- True to print the loss every 100 steps\n","    Returns:\n","    params -- dictionary containing the weights w and bias b\n","    grads -- dictionary containing the gradients of the weights and\n","    bias with respect to the cost function\n","    costs -- list of all the costs computed during the optimization,\n","    this will be used to plot the learning curve.\n","    Tips:\n","    You basically need to write down two steps and iterate through\n","    them:\n","    1) Calculate the cost and the gradient for the current\n","    parameters. Use propagate().\n","    2) Update the parameters using gradient descent rule for w and\n","    b.\n","    \"\"\"\n","    costs = []\n","    for i in range(num_iterations):\n","    # Cost and gradient calculation (� 1-4 lines of code)\n","    ### START CODE HERE ###\n","        grads, cost = propagate(w, b, X, Y)\n","        ### END CODE HERE ###\n","        # Retrieve derivatives from grads\n","        dw = grads[\"dw\"]\n","        db = grads[\"db\"]\n","        # update rule (� 2 lines of code)\n","        ### START CODE HERE ###\n","        \n","        w=w-learning_rate*dw\n","        b=b-learning_rate*db\n","        \n","        \n","        \n","        ### END CODE HERE ###\n","        # Record the costs\n","        if i % 100 == 0:\n","            costs.append(cost)\n","        # Print the cost every 100 training examples\n","        if print_cost and i % 100 == 0:\n","            print (\"Cost after iteration %i: %f\" %(i, cost))\n","    params = {\"w\": w,\n","    \"b\": b}\n","    grads = {\"dw\": dw,\n","    \"db\": db}\n","    return params, grads, costs\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7B3PAyEj06bG","colab_type":"code","colab":{}},"source":["w, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]])\n","params, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n","\n","print (\"w = \" + str(params[\"w\"]))\n","print (\"b = \" + str(params[\"b\"]))\n","print (\"dw = \" + str(grads[\"dw\"]))\n","print (\"db = \" + str(grads[\"db\"]))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"R-XSZtO006bU","colab_type":"code","colab":{}},"source":["def predict(w, b, X):\n","    '''\n","    Predict whether the label is 0 or 1 using learned logistic regression\n","    , →\n","    parameters (w, b)\n","    Arguments:\n","    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n","    b -- bias, a scalar\n","    X -- data of size (num_px * num_px * 3, number of examples)\n","    , →\n","    Returns:\n","    Y_prediction -- a numpy array (vector) containing all predictions\n","    (0/1) for the examples in X\n","    '''\n","    m = X.shape[1]\n","    Y_prediction = np.zeros((1,m))\n","    w = w.reshape(X.shape[0], 1)\n","    # Compute vector \"A\" predicting the probabilities of a cat being present in the picture\n","\n","    A = sigmoid(np.dot(w.T,X)+b)\n","\n","    for i in range(A.shape[1]):\n","    # Convert probabilities A[0,i] to actual predictions p[0,i]\n","\n","        if A[0][i]<=0.5:A[0][i]=0\n","        else: A[0][i]=1\n","    Y_prediction=A\n","\n","    assert(Y_prediction.shape == (1, m))\n","    return Y_prediction"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JidjCmpH06be","colab_type":"code","colab":{}},"source":["# GRADED FUNCTION: model\n","def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n","    \"\"\"\n","    Builds the logistic regression model by calling the function you've implemented previously\n","\n","    Arguments:\n","    X_train -- training set represented by a numpy array of shape\n","    (num_px * num_px * 3, m_train)\n","    Y_train -- training labels represented by a numpy array (vector) of\n","    shape (1, m_train)\n","    X_test -- test set represented by a numpy array of shape (num_px *\n","    num_px * 3, m_test)\n","    Y_test -- test labels represented by a numpy array (vector) of\n","    shape (1, m_test)\n","    num_iterations -- hyperparameter representing the number of\n","    iterations to optimize the parameters\n","    learning_rate -- hyperparameter representing the learning rate used\n","    in the update rule of optimize()\n","    print_cost -- Set to true to print the cost every 100 iterations\n","    Returns:\n","    d -- dictionary containing information about the model.\n","    \"\"\"\n","    ### START CODE HERE ###\n","    # initialize parameters with zeros (� 1 line of code)\n","    w, b = initialize_with_zeros(X_train.shape[0])\n","    # Gradient descent (� 1 line of code)\n","    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n","    # Retrieve parameters w and b from dictionary \"parameters\"\n","    w = parameters[\"w\"]\n","    b = parameters[\"b\"]\n","    # Predict test/train set examples (� 2 lines of code)\n","    Y_prediction_test = predict(w, b, X_test)\n","    Y_prediction_train = predict(w, b, X_train)\n","    ### END CODE HERE ###\n","    # Print train/test Errors\n","    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n","    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n","    d = {\"costs\": costs,\n","    \"Y_prediction_test\": Y_prediction_test,\n","    \"Y_prediction_train\" : Y_prediction_train,\n","    \"w\" : w,\n","    \"b\" : b,\n","    \"learning_rate\" : learning_rate,\n","    \"num_iterations\": num_iterations}\n","    return d"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GoHkUV0V06b-","colab_type":"code","colab":{}},"source":["d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost = True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IrCKrwTa2aWY","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}